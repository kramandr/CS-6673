import math
import random
import re
import unidecode
import time
from collections import defaultdict
'''
The main function below will train one Markov model for each training dataset
'''
TRAINING_DATASETS = ["Count of Monte Cristo.txt", "Secret Garden.txt", "Scarlet Letter.txt",
                     "Pride and Prejudice.txt", "Green Eggs and Ham.txt"]

'''
This is the number of sequences that will be generated by each trained model
'''
NUM_SEQS_TO_GENERATE = 10

'''
The main function below will compute probabilities from each trained Markov model (see note for TRAINING_DATASETS)
for each test dataset
'''
TEST_DATASETS = ["A Little Princess.txt", "Sense and Sensibility.txt", "The House of Seven Gables.txt",
                 "The Three Musketeers.txt"]

'''
This is where you can adjust the Markov order that is used to train all models in the main function
'''
MARKOV_ORDER = 1


'''
This variable will be set in the main function and is to be used to calculate the probability of out-of-vocabulary (OOV)
tokens when computing the probability of a sequence in which a token is found that was not encountered during training
'''
VOCAB_SIZE = None


class MarkovModel:

    def __init__(self, training_sentences, order=1):
        """
        a dictionary that maps a tuple t (whose length is the Markov order) to the probability that
        t starts a sentence

        It is recommended that you use this datastructure initially to tally counts during training and then
        convert these counts to probabilities when you normalize after training
        """
        
        self.start_probabilities = {}

        """
        a 2D dictionary that maps a tuple s (prev token) to a tuple t (next token) to the probability that
        t follows s in a sentence (use None as the second key to represent an end of sequence token)

        It is recommended that you use this datastructure initially to tally counts during training and then
        convert these counts to probabilities when you normalize after training
        """
        self.trans_probabilities = {}

        """"
        this is a class variable that represents the Markov order of the model which you can reference in class
        functions
        """
        self.order = order

        '''Here is where you initialize the model (i.e., train parameters) from the input training sentences'''
        start_time = time.time()

        # TODO: implement this constructor
        for sentence in training_sentences:
            
            if len(sentence) < order:
                continue
            
            start_token = tuple(sentence[0:order])
            if start_token in self.start_probabilities:
                self.start_probabilities[start_token] += 1.0
            else:
                self.start_probabilities[start_token] = 1.0

            for i in range(order):
                sentence.append(None)
    
            for i in range(len(sentence) - order):
                from_token = tuple(sentence[i:i+order])
                to_token = tuple(sentence[i+1:i+1+order])
                if from_token in self.trans_probabilities:
                    inner_dict = self.trans_probabilities[from_token]
                    if to_token in inner_dict:
                        inner_dict[to_token] += 1.0
                    else:
                        inner_dict[to_token] = 1.0
                else:
                    self.trans_probabilities[from_token] = {to_token: 1.0}
            
            for i in range(order):
                sentence.pop()
            
        #normalize
        start_probs_sum = sum(self.start_probabilities.values())
        for word in self.start_probabilities:
            self.start_probabilities[word] /= start_probs_sum
        
        for from_token in self.trans_probabilities:
            inner_dict = self.trans_probabilities[from_token]
            trans_prob_sum = sum(inner_dict.values())
            for to_token in inner_dict:
                inner_dict[to_token] /= trans_prob_sum

        end_time = time.time()
        self.training_time = end_time - start_time

        # Storing model size details
        self.start_prob_size = len(self.start_probabilities)
        self.trans_prob_size = sum(len(inner) for inner in self.trans_probabilities.values())

        print(f"Training completed in {self.training_time:.4f} seconds.")
        print(f"Start probabilities size: {self.start_prob_size}")
        print(f"Transition probabilities size: {self.trans_prob_size}")

    def generate_sequence(self):
        """
        probabilistically generate a sequence of words and return it as a string

        :return: a generated string
        """
        seq = []

        prev_token = sample(self.start_probabilities)
        seq.extend(prev_token)

        next_token = sample(self.trans_probabilities[prev_token])
        while next_token[-1] is not None:
            seq.append(next_token[-1])
            prev_token = next_token
            next_token = sample(self.trans_probabilities[prev_token])
        return seq

    def compute_log_probability(self, test_sentences):
        """
        Compute the log probability of the input text according to the trained model.

        :param test_sentences: List of tokenized sentences (each sentence is a list of words)
        :return: Log probability of the text
        """
        log_probability = 0.0
        OOV_log_prob = math.log(1.0 / VOCAB_SIZE)

        for sentence in test_sentences:
            if len(sentence) < self.order:
                continue  # 

            # Compute log probability of the start sequence
            start_tuple = tuple(sentence[0:self.order])
            if start_tuple in self.start_probabilities:
                log_probability += math.log(self.start_probabilities[start_tuple])
            else:
                log_probability += OOV_log_prob

            # Compute log probability for transitions
            for i in range(len(sentence) - self.order):
                prev_token = tuple(sentence[i:i + self.order])  # Previous state (context)
                next_token = tuple(sentence[i + 1: i + 1 + self.order])  # Next token to predict

                if prev_token in self.trans_probabilities:
                    inner_dict = self.trans_probabilities[prev_token]
                    if next_token in inner_dict:
                        log_probability += math.log(inner_dict[next_token])
                    else:
                        log_probability += OOV_log_prob
                else:
                    log_probability += OOV_log_prob

            for i in range(self.order):
                sentence.pop()

        return log_probability

def sample(distribution):
    """
        Given a dictionary that maps a tuple t to a probability, this function samples and returns t according to
        its probability

    :param distribution: The dictionary from which to sample
    :return: a tuple
    """
    prob = random.random()
    prob_sum = 0.0

    for key, value in distribution.items():
        prob_sum += value
        if prob_sum >= prob:
            return key

    return key


def parse_file(filename):
    """
    Takes in the name of a file which is parsed and the parsed input cleaned
    Returns a list of lists of strings where each list of strings
    represents a sentence of words (each word is a string)
    """
    filename_w_path = "./data/" + filename
    with open(filename_w_path, 'r', encoding='utf-8-sig') as file:
        sentences = re.split('[;:!?.]', unidecode.unidecode(file.read()).lower())
        sentences = [[word.strip("[]()_-\'\", \n\t")
                      for word in sentence.strip("[]()_-\'\", \n\t").replace('-', ' ').split()]
                     for sentence in sentences]
        sentences = [sentence for sentence in sentences if sentence != []]
    return sentences


def compute_vocab_size():
    vocab = set()
    for TRAINING_DATASET in TRAINING_DATASETS:
        sentences = parse_file(TRAINING_DATASET)
        for sentence in sentences:
            vocab.update(sentence)

    for TEST_DATASET in TEST_DATASETS:
        sentences = parse_file(TEST_DATASET)
        for sentence in sentences:
            vocab.update(sentence)

    return len(vocab)


if __name__ == '__main__':

    VOCAB_SIZE = compute_vocab_size()

    markov_models = {}
    for TRAINING_DATASET in TRAINING_DATASETS:
        # Train a model from scratch
        print("Training new model on", TRAINING_DATASET, "...")
        train_file_sentences = parse_file(TRAINING_DATASET)
        markov_models[TRAINING_DATASET] = MarkovModel(train_file_sentences, MARKOV_ORDER)

    for TRAINING_DATASET in TRAINING_DATASETS:

        if NUM_SEQS_TO_GENERATE > 0:
            print("\nGenerating", NUM_SEQS_TO_GENERATE, "sequences from the model trained on", TRAINING_DATASET)
            for i in range(NUM_SEQS_TO_GENERATE):
                print("\t", markov_models[TRAINING_DATASET].generate_sequence())

    for TEST_DATASET in TEST_DATASETS:
        test_file_sentences = parse_file(TEST_DATASET)

        print("\nThe log probability of test dataset", TEST_DATASET, "is (less negative means more probable)")
        for model_name, markov_model in markov_models.items():
            probability = markov_model.compute_log_probability(test_file_sentences)
            print("\t", probability, "according to the model trained on", model_name)