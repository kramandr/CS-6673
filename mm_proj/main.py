import math
import random
import re
import unidecode
from collections import defaultdict
'''
The main function below will train one Markov model for each training dataset
'''
TRAINING_DATASETS = ["Count of Monte Cristo.txt", "Secret Garden.txt", "Scarlet Letter.txt",
                     "Pride and Prejudice.txt", "Green Eggs and Ham.txt"]

'''
This is the number of sequences that will be generated by each trained model
'''
NUM_SEQS_TO_GENERATE = 10

'''
The main function below will compute probabilities from each trained Markov model (see note for TRAINING_DATASETS)
for each test dataset
'''
TEST_DATASETS = ["A Little Princess.txt", "Sense and Sensibility.txt", "The House of Seven Gables.txt",
                 "The Three Musketeers.txt"]

'''
This is where you can adjust the Markov order that is used to train all models in the main function
'''
MARKOV_ORDER = 2

'''
This variable will be set in the main function and is to be used to calculate the probability of out-of-vocabulary (OOV)
tokens when computing the probability of a sequence in which a token is found that was not encountered during training
'''
VOCAB_SIZE = None


class MarkovModel:

    def __init__(self, training_sentences, order=1):
        """
        Initialize the Markov model with training sentences.
        """
        self.order = order
        self.start_prob_counts = defaultdict(int)  # Counts before normalization
        self.trans_prob_counts = defaultdict(lambda: defaultdict(int))  # Transition counts
        self.vocab = set()

        # Initialize empty dictionaries for storing final probabilities
        self.start_probabilities = {}  
        self.trans_probabilities = {}

        for sentence in training_sentences:

            sentence = [word.lower() for word in sentence]
            if len(sentence) < order:
                continue  # Skip short sentences
            
            start_tuple = tuple(sentence[:order])
            self.start_prob_counts[start_tuple] += 1
            
            # Count transitions
            for i in range(len(sentence) - order):
                current_state = tuple(sentence[i:i+order])
                next_state = tuple(sentence[i+1:i+order+1])
                self.trans_prob_counts[current_state][next_state] += 1
                self.vocab.update(current_state)
            
            # Count end-of-sequence transitions
            last_state = tuple(sentence[-order:])
            self.trans_prob_counts[last_state][None] += 1  # None represents sentence end

        # Convert raw counts to probability distributions
        self.start_probabilities = self._normalize_counts(self.start_prob_counts)
        self.trans_probabilities = {
            state: self._normalize_counts(next_states)
            for state, next_states in self.trans_prob_counts.items()
        }

    def _normalize_counts(self, count_dict):
            total = sum(count_dict.values())
            return {k: v / total for k, v in count_dict.items()}

    def generate_sequence(self, max_length=20):
        """
        Generate a probabilistic sequence of words based on the trained Markov model.

        :param max_length: The maximum length of the generated sequence.
        :return: A generated string sequence.
        """
        if not self.start_probabilities:
            return ""  # No data to generate from

        current_tuple = sample(self.start_probabilities)
        sequence = list(current_tuple)

        for _ in range(max_length - self.order): 
            if current_tuple not in self.trans_probabilities:
                break 

            next_word = sample(self.trans_probabilities[current_tuple])
            if next_word is None: 
                break

            sequence.append(next_word if isinstance(next_word, str) else next_word[-1])
            current_tuple = tuple(sequence[-self.order:])  

        return " ".join(sequence) 



    def compute_log_probability(self, test_sentences):
        """
        Compute the log probability of test sentences using the trained Markov model.

        :param test_sentences: A list of sentences (each sentence is a list of words)
        :return: The computed log probability
        """
        log_probability = 0.0
        vocab_size = len(self.vocab)  # Number of unique words in the training data
        smoothing_prob = 1 / vocab_size  # Smoothing for unseen transitions
        log_smoothing = math.log(smoothing_prob)

        for sentence in test_sentences:
            if len(sentence) < self.order:
                continue  # Skip short sentences

            # Compute log probability for the start sequence
            start_tuple = tuple(sentence[:self.order])
            start_p = self.start_probabilities.get(start_tuple, smoothing_prob)
            log_probability += math.log(start_p)

            # Compute transition probabilities for the sentence
            for i in range(len(sentence) - self.order):
                current_state = tuple(sentence[i:i + self.order])
                next_state = tuple(sentence[i + 1:i + self.order + 1])

                if current_state in self.trans_probabilities:
                    trans_p = self.trans_probabilities[current_state].get(next_state, smoothing_prob)
                else:
                    trans_p = smoothing_prob

                log_probability += math.log(trans_p)

            # Handle end-of-sequence probability
            last_state = tuple(sentence[-self.order:])
            end_p = self.trans_probabilities.get(last_state, {}).get(None, smoothing_prob)
            log_probability += math.log(end_p)

        return log_probability


def sample(distribution):
    """
        Given a dictionary that maps a tuple t to a probability, this function samples and returns t according to
        its probability

    :param distribution: The dictionary from which to sample
    :return: a tuple
    """
    prob = random.random()
    prob_sum = 0.0

    for key, value in distribution.items():
        prob_sum += value
        if prob_sum >= prob:
            return key

    return key


def parse_file(filename):
    """
    Takes in the name of a file which is parsed and the parsed input cleaned
    Returns a list of lists of strings where each list of strings
    represents a sentence of words (each word is a string)
    """
    filename_w_path = "./data/" + filename
    with open(filename_w_path, 'r', encoding='utf-8-sig') as file:
        sentences = re.split('[;:!?.]', unidecode.unidecode(file.read()).lower())
        sentences = [[word.strip("[]()_-\'\", \n\t")
                      for word in sentence.strip("[]()_-\'\", \n\t").replace('-', ' ').split()]
                     for sentence in sentences]
        sentences = [sentence for sentence in sentences if sentence != []]
    return sentences


def compute_vocab_size():
    vocab = set()
    for TRAINING_DATASET in TRAINING_DATASETS:
        sentences = parse_file(TRAINING_DATASET)
        for sentence in sentences:
            vocab.update(sentence)

    for TEST_DATASET in TEST_DATASETS:
        sentences = parse_file(TEST_DATASET)
        for sentence in sentences:
            vocab.update(sentence)

    return len(vocab)


if __name__ == '__main__':

    VOCAB_SIZE = compute_vocab_size()

    markov_models = {}
    for TRAINING_DATASET in TRAINING_DATASETS:
        # Train a model from scratch
        print("Training new model on", TRAINING_DATASET, "...")
        train_file_sentences = parse_file(TRAINING_DATASET)
        markov_models[TRAINING_DATASET] = MarkovModel(train_file_sentences, MARKOV_ORDER)

    for TRAINING_DATASET in TRAINING_DATASETS:

        if NUM_SEQS_TO_GENERATE > 0:
            print("\nGenerating", NUM_SEQS_TO_GENERATE, "sequences from the model trained on", TRAINING_DATASET)
            for i in range(NUM_SEQS_TO_GENERATE):
                print("\t", markov_models[TRAINING_DATASET].generate_sequence())

    for TEST_DATASET in TEST_DATASETS:
        test_file_sentences = parse_file(TEST_DATASET)

        print("\nThe log probability of test dataset", TEST_DATASET, "is (less negative means more probable)")
        for model_name, markov_model in markov_models.items():
            probability = markov_model.compute_log_probability(test_file_sentences)
            print("\t", probability, "according to the model trained on", model_name)